# Lesson 1.1: What is Prompting?

Welcome to the first lesson of our Prompt Engineering course. In this lesson, we'll explore the fundamentals of prompting and its importance in communicating with Large Language Models (LLMs).

## Basics of Prompting an LLM

Prompting is the process of providing input to an AI model to generate a desired output. When working with Large Language Models (LLMs), prompts are the primary means of interaction. You can achieve a lot with simple prompts, but the quality of results depends on how much information you provide and how well-crafted the prompt is.

A prompt can contain various elements, including:
- The instruction or question you're passing to the model
- Context or background information
- Input data
- Examples of desired output

By effectively using these elements, you can instruct the model more precisely and improve the quality of the results.

### Simple Prompt Example

Let's start with a basic example of a simple prompt:

"Write a Python function to calculate rectangle area. Assume length and width are integers."

![Example output from GitHub Copilot for a simple rectangle area function](../assets/code1.png)

Prompt: "Write a Python function to calculate the area of a rectangle, given its length and width. Include input validation and error handling. Provide an example usage."
Breakdown:

![Example output from GitHub Copilot for a more complex rectangle area function](../assets/code2.png)

1. Instruction: "Write a Python function"
2. Context: "to calculate the area of a rectangle"
3. Input Data: "given its length and width"
4. Desired Output: "Include input validation and error handling. Provide an example usage."
